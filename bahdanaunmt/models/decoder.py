import torch
from torch import nn
import torch.nn.functional as F


class Bahdanau(nn.Module):
    """Implementation of additive attention (Bahdanau attention) as described in "Neural Machine Translation by Jointly Learning to Align and Translate" by Bahdanau, et al.

        Learns to assign attention weigts to each word annotation generated by the encoder when predicting a new word.
    """
    def __init__(self, hidden_size, *args, **kwargs) -> None:
        """
        Args:
            hidden_size (int): Size of each word annotation and decoder hidden state
        """
        super().__init__(*args, **kwargs)
        self.W_q = nn.Linear(hidden_size, hidden_size, bias=False)
        self.W_k = nn.Linear(hidden_size, hidden_size, bias=False)
        self.V = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, query, keys):
        """
        Args:
            query (torch.Tensor): Query vector, Bahdanau attention defines it as the previous state of the decoder
            keys (torch.Tensor): Keys matrix, Bahdanau attention defines it as the encoder outputs (input sentence annotation)
        """
        query_R = query.repeat(1, keys.size(1), 1)
        energy = torch.tanh(self.W_q(query_R) + self.W_k(keys))
        scores = self.V(energy)
        attention_weights = F.softmax(scores, dim=1)

        context = torch.sum((attention_weights * keys), dim=1)

        return context, attention_weights
    

class Decoder(nn.Module):
    """Implementation fo a basic RNN decoder with additive attention and GRU cells.
    """
    def __init__(self, hidden_size, output_size, dropout_p=0.1, device='cuda', SOS_Token=0, MAX_LENGTH=15) -> None:
        """
        Args:
            hidden_size (int): Size of each encoder word annotation and decoder hitten states
            output_size (int): Size of target language vocabulary
            dropit_p (float) = 0.1: Dropout percentage
            device (str) = 'cuda': Device where the model runs
            SOS_Token (int) = 0: Start-of-Sequence token
            MAX_LENGTH (int) = 15: Maximum sentence length.
        """
        super(Decoder, self).__init__()

        self.device = device
        self.SOS_Token = SOS_Token
        self.MAX_LENGTH = MAX_LENGTH

        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.bahdanau = Bahdanau(hidden_size)
        self.gru = nn.GRU(hidden_size + hidden_size, hidden_size, batch_first=True)
        self.out = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(dropout_p)

    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):
        """
        Args:
            encoder_outputs (torch.Tensor): Encoder output
            encoder_hidden (torch.Tensor): Final encoder hidden state
            target_tensor (torch.Tensor) = None: Tensor of target, for teacher forcing
        """
        batch_size = encoder_outputs.size(0)
        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=self.device).fill_(self.SOS_Token)
        decoder_hidden = encoder_hidden
        decoder_outputs = []
        attentions = []
        

        for i in range(self.MAX_LENGTH):
            decoder_output, decoder_hidden, attn_weights = self.forward_step(
                decoder_input, decoder_hidden, encoder_outputs
            )
            decoder_outputs.append(decoder_output)
            attentions.append(attn_weights)

            if target_tensor is not None:
                decoder_input = target_tensor[:, i].unsqueeze(1)
            else:
                _, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze(-1).detach()
        
        decoder_outputs = torch.cat(decoder_outputs, dim=1)
        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)
        attentions = torch.cat(attentions, dim=0)

        return decoder_outputs, decoder_hidden, attentions


    def forward_step(self, input, hidden, encoder_outputs):
        embedded = self.dropout(self.embedding(input))

        hidden = hidden.view(encoder_outputs.size(0), 1, self.hidden_size)
        context, attention_weights = self.bahdanau(hidden, encoder_outputs)
        hidden = hidden.view(1, encoder_outputs.size(0), self.hidden_size)
        context = context.unsqueeze(1)
        input_gru = torch.cat((embedded, context), dim=2)

        output, hidden = self.gru(input_gru, hidden)
        output = self.out(output)

        return output, hidden, attention_weights